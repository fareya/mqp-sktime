{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sktime.transformers.compose import ColumnConcatenator\n",
    "from sktime.classifiers.compose import TimeSeriesForestClassifier\n",
    "from sktime.classifiers.dictionary_based.boss import BOSSEnsemble\n",
    "from sktime.classifiers.compose import ColumnEnsembleClassifier\n",
    "from sktime.classifiers.shapelet_based import ShapeletTransformClassifier\n",
    "from sktime.datasets import load_basic_motions\n",
    "from sktime.pipeline import Pipeline\n",
    "\n",
    "from sktime.classifiers.distance_based import ProximityForest\n",
    "from sktime.classifiers.distance_based import KNeighborsTimeSeriesClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "#cleaning up whatever happened in loading data \n",
    "#feed in the data \n",
    "from sktime.utils.load_data import from_long_to_nested\n",
    "import time \n",
    "\n",
    "#setting constants \n",
    "classifer={\n",
    "    'TSF_CLF':0,\n",
    "    'PF_CLF':1,\n",
    "    'KNN_CLF':2\n",
    "}\n",
    "\n",
    "\n",
    "#concatenates ....\n",
    "#make this assumption cleare \n",
    "#write better columns \n",
    "#error messages\n",
    "#define constants when refactoring(readability)\n",
    "\n",
    "#extracting the data from the csv as a dataframe and reformatting it \n",
    "\n",
    "def reformatData(target, file_name):\n",
    "    raw_df= pd.read_csv(file_name)\n",
    "    \n",
    "    #collapses the time cols \n",
    "    long_table_df= raw_df.melt(id_vars=[\"event\", \"name\",\"start time\", \"end time\",\"channel\"], \n",
    "            var_name=\"anindex\", \n",
    "            value_name=\"value\")\n",
    "\n",
    "    sorted_long_table_df=long_table_df.sort_values(by=['event','name','start time','channel'], axis=0)\n",
    "\n",
    "    unique_dim_ids = sorted_long_table_df.iloc[:, 4].unique()\n",
    "\n",
    "    for i in range(len(unique_dim_ids)):\n",
    "        my_channel=unique_dim_ids[i]\n",
    "        sorted_long_table_df['channel']=sorted_long_table_df['channel'].replace({my_channel:i})\n",
    "    unique_start_time = sorted_long_table_df.iloc[:, 2].unique()\n",
    "\n",
    "    for i in range(len(unique_start_time)):\n",
    "        my_time=unique_start_time[i]\n",
    "        sorted_long_table_df['start time']=sorted_long_table_df['start time'].replace({my_time:i})\n",
    "\n",
    "    \n",
    "    sorted_long_table_df_stripped=sorted_long_table_df.drop(columns=['event','name','end time'])\n",
    "\n",
    "    sorted_long_table_df_stripped.head()\n",
    "    df_nested = from_long_to_nested(sorted_long_table_df_stripped)\n",
    "\n",
    "\n",
    "    target='event'\n",
    "    new_unique_start_time=sorted_long_table_df.iloc[:, 2].unique()\n",
    "    labels=[]\n",
    "    for e in new_unique_start_time:\n",
    "        x=sorted_long_table_df.loc[sorted_long_table_df['start time']==e,[target]].iloc[0][0]\n",
    "        labels.append(x)\n",
    "\n",
    "    np_labels= np.asarray(labels, dtype=np.str)\n",
    "    \n",
    "    return df_nested, np_labels \n",
    "\n",
    "def splitTestTrain(X, y, percent_train):\n",
    "    msk = np.random.rand(len(X)) < percent_train\n",
    "    ytrain=y[msk]\n",
    "    ytest=y[~msk]\n",
    "    Xtrain=X[msk]\n",
    "    Xtest=X[~msk]\n",
    "    \n",
    "    return Xtrain, ytrain, Xtest, ytest\n",
    "\n",
    "def concatenateMethod(Classifier, x_train, y_train, x_test, y_test):\n",
    "    steps = [\n",
    "    ('concatenate', ColumnConcatenator()),\n",
    "    ('classify', Classifier(n_estimators=10))]\n",
    "    clf = Pipeline(steps)\n",
    "    clf.fit(x_train, y_train)\n",
    "    return clf.score(x_test, y_test)\n",
    "\n",
    "def concatenateMethodTake2(classifier_num, X, y, percent_train):\n",
    "    concatenation_instance=ColumnConcatenator()\n",
    "    X_concatenated=concatenation_instance.fit(X).transform(X)\n",
    "    Xtrain, ytrain, Xtest, ytest= splitTestTrain(X_concatenated,y,percent_train)\n",
    "    \n",
    "    if(classifier_num==0):\n",
    "        clf= TimeSeriesForestClassifier(n_estimators=10)\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        return clf.score(Xtest,ytest)\n",
    "    elif (classifier_num==1):\n",
    "        knn = KNeighborsTimeSeriesClassifier(metric='dtw')\n",
    "        knn.fit(Xtrain, ytrain)\n",
    "        return knn.score(Xtest, ytest)\n",
    "    elif (classifier_num == 2):    \n",
    "        pf = ProximityForest(n_trees=10)\n",
    "        pf.fit(Xtrain, ytrain)\n",
    "        return pf.score(Xtest, ytest)\n",
    "    else: \n",
    "        return 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target='event'\n",
    "file_name=\"../scripts/data/2013e.csv\"\n",
    "X, y=reformatData(target,file_name)\n",
    "percent_train=0.8\n",
    "should_profile=True  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4166666666666667\n"
     ]
    }
   ],
   "source": [
    "#running time series forest classifier \n",
    "acc= concatenateMethodTake2(0, X, y, 0.8)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k nearest neighbors \n",
    "acc= concatenateMethodTake2(1, X[[\"dim_0\", \"dim_1\",\"dim_2\"]], y, 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4642857142857143\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#proximity forest \n",
    "acc= concatenateMethodTake2(2, X[[\"dim_0\", \"dim_1\"]], y, 0.8)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "def main():\n",
    "    # this is where method selection will take place and the configuration files will be parsed \n",
    "    #numbers of models to run \n",
    "    start_time=time.time()\n",
    "    print(\"hello\")\n",
    "    end_time= time.time() -start_time\n",
    "    print(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "0.0003101825714111328\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
